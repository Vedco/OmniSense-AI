# OmniSense-AI (Text + Image + Audio + Video)
An AI-powered multimodal assistant enabling real-time text, voice, image, and video understanding.

An advanced Multimodal AI Assistant capable of understanding and responding to multiple input formats including text, speech, images, and videos using modern Generative AI models.

This project demonstrates applied AI engineering by integrating speech recognition, computer vision, large language models (LLMs), and text-to-speech systems into a unified intelligent assistant.

---

## ğŸŒŸ Features

- âœ… Text-based AI conversations
- âœ… Voice input (Speech-to-Text)
- âœ… Text-to-Speech responses
- âœ… Image understanding & visual reasoning
- âœ… Video summarization
- âœ… Modular architecture
- âœ… Real-time multimodal interaction

---

## ğŸ§  How It Works

The assistant follows a multimodal processing pipeline:

### 1ï¸âƒ£ Input Layer
- Text input
- Voice input (microphone/audio file)
- Image upload
- Video upload

### 2ï¸âƒ£ Preprocessing Layer
- Speech converted to text
- Images processed using vision models
- Video frames extracted & analyzed

### 3ï¸âƒ£ AI Processing Layer
- Processed input sent to LLM (Gemini / GPT APIs)
- Model generates contextual response

### 4ï¸âƒ£ Output Layer
- Text response displayed
- Optional speech output using Text-to-Speech (gTTS)

---

## ğŸ— Project Architecture

```
User Input
   â†“
Input Handler (Text / Audio / Image / Video)
   â†“
Preprocessing (STT / Vision / Frame Extraction)
   â†“
LLM Processing
   â†“
Response Generation
   â†“
Text Output + Optional Speech Output
```

---

## ğŸ“‚ Project Structure

```
Multimodal-AI-Assistant/
â”‚
â”œâ”€â”€ app.py                    # Main application controller
â”œâ”€â”€ audio_handler.py          # Handles audio file processing
â”œâ”€â”€ image_handler.py          # Handles image processing
â”œâ”€â”€ video_handler.py          # Handles video analysis
â”œâ”€â”€ voice_input.py            # Microphone input handling
â”œâ”€â”€ text_to_speech_gTTS.py    # Converts text responses to speech
â”œâ”€â”€ helper.py                 # Utility functions
â”œâ”€â”€ handlers_imports.py       # Centralized imports
â”œâ”€â”€ requirements.txt          # Project dependencies
â””â”€â”€ README.md                 # Project documentation
```

---

## ğŸ›  Tech Stack

- Python
- Generative AI APIs (Gemini / GPT)
- SpeechRecognition
- gTTS (Text-to-Speech)
- OpenCV
- PIL
- Streamlit / UI framework

---

## ğŸ”§ Installation

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/yourusername/multimodal-ai-assistant.git
cd multimodal-ai-assistant
```

### 2ï¸âƒ£ Create virtual environment
```bash
python -m venv venv
source venv/bin/activate   # Mac/Linux
venv\Scripts\activate      # Windows
```

### 3ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Run the application
```bash
python app.py
```

---

## ğŸ¯ Example Use Cases

- AI-powered learning assistant
- Smart document summarizer
- Visual content analyzer
- Voice-enabled chatbot
- Video summarization tool

---

## ğŸš€ Future Enhancements

- Add Retrieval-Augmented Generation (RAG)
- Implement conversation memory
- Convert to FastAPI backend
- Docker containerization
- Cloud deployment (AWS / GCP)
- Add real-time streaming responses

---

## ğŸ“ˆ Skills Demonstrated

- Multimodal AI system design
- Large Language Model integration
- Speech-to-Text and Text-to-Speech pipelines
- Computer Vision + NLP integration
- Modular software architecture
- End-to-end AI application development

---

## ğŸ“œ License

MIT License

---

## ğŸ‘¨â€ğŸ’» Author

Ved Bhatt  
Masterâ€™s in Data Science (AI Focus)

